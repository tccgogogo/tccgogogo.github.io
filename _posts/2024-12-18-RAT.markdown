---
layout: post
title: "RAT:融合RAG和CoT的高效多步推理任务策略"
subtitle:   "RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation"
date:       2024-12-18 20:00:00
author:     "tiancc"
header-mask: 0.3
catalog:    true
tags:
    - RAG
    - COT
    - LLM
---
## 论文来源
今天分享的是由北京大学、加州大学洛杉矶分校和北京通用人工智能研究院合作发表的一篇文章

**论文题目**：RAT: Retrieval Augmented Thoughts Elicit
Context-Aware Reasoning in Long-Horizon
Generation

**论文链接**：https://arxiv.org/pdf/2403.05313

**代码地址**：https://github.com/CraftJarvis/RAT

**Demo地址**：https://huggingface.co/spaces/jeasinema/RAT



![](/img/RAT/paper.png)


## 论文概述

思维链技术通过在提示词中加入“让我们一步一步思考”的指令，模型会自动输出中间推理步骤，从而生成更加精准的答案。这种方法不仅提高了答案模型的推理能力，还增强了答案的可解释性。但是由于大模型的幻觉问题，在长任务推理中模型可能会生成看似合理但实际上并不准确的推理信息。因此这篇论文考虑在COT的基础上加上了RAG，即**RAT**，通过利用检索到的外部信息为大模型提供推理依据。RAT的核心包含两个关键思想：

- 利用 LLMs 的 zero-shot 能力生成初始思维链，并将思维链和原始任务放入提示中检索有助于修正错误思维链的信息；
- 采用渐进的方法，当前思维步骤会根据任务提示、过去的思维链信息以及检索到的文档进行修正。

![](/img/RAT/frame.png)

## 论文核心

RAT 的核心是将 RAG 用于修正由 CoT 提示生成的每一个思维步骤中，通过查找不同参考文档来不断修正每一个思维步骤，确保推理的每一步都有准确且可靠的信息支持。


![](/img/RAT/core.png)



1. **生成初始的思维**：对于给定的任务，首先根据提示“让我们逐步思考”，让 LLM  生成逐步的思维过程， 得到
$T := \left\{ T_i \right\}_{i=1}^{n}$ ，其中 $T_i$ 表示第 $i$  个思考步骤。在具体的生成任务中，T 可以是中间推理步骤（例如代码生成中的带注释的伪代码、创意写作中的文章大纲等）。
2. **使用RAG来修复大模型生成思维步骤**：假设已经修复了之前的思考步骤，现在要修复第 $i$  个思维步骤 $T_{1: i}$ ，将现在和过去的思维步骤 $\{ I, T_1, \dots, T_i \}$ 转化为将可以被LLM检索系统处理的查询，得到 $Q_i = \text{ToQuery}(I, T_1, \dots, T_i)$
3. **检索文档**：使用RAG检索 $Q_i$ 相关的文档，然后将检索后的结果附加到提示中生成修复后的思维步骤 $T_{1:i}^\star = p_\theta(\cdot \mid I, T_1, \dots, T_i, R_i)$
4. **生成答案**：根据修复后的思维步骤逐步生成答案

### Prompt细节
- 生成初始答案
```
{user}
##Question:
{question}
##Instruction:
Try to answer this question/instruction with step-by-step thoughts and make the answer more structural.
Use /n/n to split the answer into several paragraphs.
Just respond to the instruction directly. DO NOT add additional explanations or introducement in the answer unless you
are asked to.
{assistant}
```
- 生成搜索查询，相当于RAG的input

```
##Question:
{question}
##Content:
{answer}
##Instruction:
I want to verify the content correctness of the given question, especially the last sentences.
Please summarize the content with the corresponding question.
This summarization will be used as a query to search with Bing search engine.
The query should be short but need to be specific to promise Bing can find related knowledge or pages.
You can also use search syntax to make the query short and clear enough for the search engine to find relevant language
data.
Try to make the query as relevant as possible to the last few sentences in the content.
**IMPORTANT**
Just output the query directly. DO NOT add additional explanations or introducement in the answer unless you are
asked to.
{assistant}
```
- 根据检索到的上下文修正思维

```
{user}
##Existing Text in Wiki Web:
{content}
##Question:
{question}
##Answer:
{answer}
##Instruction:
I want to revise the answer according to retrieved related text of the question in WIKI pages.
You need to check whether the answer is correct.
If you find some errors in the answer, revise the answer to make it better.
If you find some necessary details are ignored, add it to make the answer more plausible according to the related text.
If you find the answer is right and do not need to add more details, just output the original answer directly.
**IMPORTANT**
Try to keep the structure (multiple paragraphs with its subtitles) in the revised answer and make it more structural
for understanding. Split the paragraphs with /n/n characters. Just output the revised answer directly. DO NOT add
additional explanations or annoucement in the revised answer unless you are asked to.
{assistant}
```

### 对话示例

![](/img/RAT/example.png)


## 论文实验

**论文采用了四组基准测试来衡量RAT的效果**：

- 代码生成：包括 HumanEval、HumanEval+、MBPP 和 MBPP+。这些基准测试涵盖了广泛的编程问题，从简单的函数实现到更复杂的算法挑战，为评估生成能力提供了强大的测试平台。采用经典的通过率 pass@k 作为评估指标。
- 数学推理评估：在 GSM8K 和 GSM-HARD 数据集上进行的，这些数据集包含了成千上万的多步骤数学问题。采用准确率作为评估指标。
- 创意写作任务：评估 RAT 的多样性，包括调查、总结等，突显开放式文本生成不同方面。进行了人类评估计算 true_skill 评分。
- 实体规划任务：从简单目标到具有挑战性的钻石目标等100个任务，类似创意写作任务，通过 MC-TextWorld 进行评估。

**论文采用的baseline**：为了公平比较，实验包括了一系列基线方法，如直接生成（DIRECT）、CoT提示方法、RAG方法（单次和多次检索配置）

**模型和设置**：实验测试了不同规模的LLM，包括GPT-3.5、GPT-4和CodeLLaMA-7b，所有实验均在零样本（zero-shot）设置下进行。

**RAT设置**：RAT 利用检索增强生成方法的功能，通过整合外部知识源来提高语言模型的性能。对于代码生成和数学推理任务，采用 codeparrot/github-jupyter 数据集作为本文的主要搜索向量库。对于 Minecraft 中的具身规划任务，使用了 Minecraft Wiki1 和 DigMinecraft2 网站作为 LLMs 可访问的信息源。对于开放式的创意写作任务，使用 Google 在互联网上搜索查询。使用了 OpenAI 的 text-embedding-ada-002 API 服务，用于不同方法和基础模型的所有嵌入计算。

![](/img/RAT/result.png)

## 论文总结

这篇文章提出的RAT结合了RAG和CTO思想，使用RAG检索的文档动态优化COT中的每一个步骤，确保每一个推理步骤都是有依据的，避免大模型的幻觉。RAT在多种生成任务上进行了验证，包括代码生成、数学推理、创造性写作和体现任务规划等。实验结果表明，RAT在这些任务上相比传统的CoT提示和RAG方法都有显著的性能提升。

但是RAT仍存在一些问题，如：RAT的性能也依赖于检索到的知识质量，如何构建和评估一个用于高效和有效检索的知识库是一个关键问题，同时迭代地修正每个推理步骤可能会增加计算成本；RAT中用到的COT思想并不适用于所有问题，对于简单问题不需要进行逐步思考，而对于复杂问题，可能需要建模成思维树或者思维图来解决。

