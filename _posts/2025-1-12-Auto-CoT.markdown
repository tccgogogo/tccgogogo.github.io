---
layout: post
title: "Auto-CoT：自动构建大模型的思维链提示"
subtitle:   "AUTOMATIC CHAIN OF THOUGHT PROMPTING IN LARGE LANGUAGE MODELS"
date:       2025-1-12 20:00:00
author:     "tiancc"
header-mask: 0.3
catalog:    true
tags:
    - RAG
    - COT
    - LLM
---
## 论文来源
今天分享的是由上海交通大学发表的一篇文章：Auto-COT

**论文题目**：AUTOMATIC CHAIN OF THOUGHT PROMPTING
IN LARGE LANGUAGE MODELS

**论文链接**：https://arxiv.org/pdf/2210.03493

**代码地址**：https://github.com/amazon-science/auto-cot


![](/img/Auto-CoT/paper.png)

## 论文概述
大模型通过思维链COT将复杂问题分解为多个中间步骤，展示出了强大的推理能力，大模型的思维链COT主要分为两大范式：
- **零样本链式思维（Zero-Shot-CoT）**：与任务无关，在测试问题之后添加一个如“让我们一步一步思考”的单一提示语，以促进LLMs中的推理链条。
- **手动链式思维（Manual-CoT）**：包含一个问题和对应的推理思维链。这是思维链是由一系列中间推理步骤（即理由）和预期答案组成。
![](/img/Auto-CoT/cot.png)

这篇文章提出了**自动思维链（Auto-CoT）**，先将问题聚类，每一类中抽出一个具有代表性的问题，利用零样本思维链为每个问题生成推理链。例如：一共有K个聚类，会抽取K个问题，用提示词“让我们一步一步地思考”，生成K套思维链步骤。当向大模型提出一个新问题后，系统将K套思维链步骤作为提示词和新问题一并输入大模型，并完成作答。

## 问题分析

这篇论文设计了一种称为 **Retrieval-Q-CoT**的方法，该方法基于余弦相似性检索最相似的前 $ k $ 个（例如 $ k = 8 $）相似的问题。为了与这种基于相似性的方法对比，使用**Random-Q-CoT**方法，为每个测试问题随机抽样其他 $ k $ 个测试问题。这两种方法都调用 Zero-Shot-CoT 来为每个抽样的问题 $ q^{demo}_i $ 生成推理链 $ c^{demo}_i $（包括理由和答案），因为 LLM 在零样本设置下是不错的推理器。

![](/img/Auto-CoT/question.png)
从实验结果来看，在调用Zero-Shot-CoT时，Retrieval-Q-CoT在算术数据集MultiArith上的表现不如Random-Q-CoT，而在带有注释推理链的GSM8K和AQuA两个数据集上，Retrieval-Q-CoT甚至优于Manual-CoT。结果表明，Retrieval-Q-CoT的较差性能是由Zero-Shot-CoT的错误推理链引起的。

### 相似性检索产生误导
相似性误导指的是，Retrieval-Q-CoT中的推理链（包括基本原理和答案）是由Zero-Shot-CoT生成的，它们可能存在导致错误的答案。在检索到与测试问题类似的问题之后，由Zero-Shot-CoT引起的错误演示可能会误导同一个LLM以类似的方式推理错误的答案。
- 探讨是否是相似性检索导致Retrieval-Q-CoT性能低

在来自MultiArith数据集的所有600个问题上调用了Zero-Shot-CoT。在这之中，我们收集了那128个Zero-Shot-CoT生成了错误答案的问题（错误率：21.3% ）。在Zero-Shot-CoT失败的问题Q中，将那些即使有了额外演示后Retrieval-Q-CoT或Random-Q-CoT仍然失败的问题称为未解决的问题。将未解决问题的数量除以128（Q中的问题数量）来计算未解决率。
Retrieval-Q-CoT的未解决率为46.9%，远高于Random-Q-CoT的25.8%。这表明当为测试问题采样了相似的问题时，Retrieval-Q-CoT受到相似性误导的负面影响更大。

![](/img/Auto-CoT/retrieval.png)

### 出错问题属于哪一类


使用K-means聚类可以帮助识别出哪些类型的问题更难被Zero-Shot-CoT正确解答。高频错误簇的存在表明，某些问题类型对于Zero-Shot-CoT来说确实更具挑战性。

Retrieval-Q-CoT依赖于找到相似的问题来构建演示，但如果这些问题是来自高频错误簇，则可能导致错误的传播。

相比之下，Random-Q-CoT由于其随机抽样的特性，能够避免因相似性而导致的错误复制，从而表现出更低的未解决率1。
![](/img/Auto-CoT/wrong-question.png)

### 多样性降低相似性产生的误导
假设所有错误演示的问题都属于同一个高频错误簇，则从每个不同的簇中抽样一个问题。由于不同的簇反映了问题语义的多样性，这种基于聚类的抽样方法可以被视为基于多样性的。

通过引入基于多样性的抽样方法以及利用简单的启发式规则，我们可以有效地减少由零样本CoT产生的误导，并提高自动化生成的推理链的质量。这种方法不仅提高了正确解答的可能性，还增强了系统对不同类型问题的适应性，从而提升了整体的推理性能。
## 论文核心

![](/img/Auto-CoT/frame.png)

Auto-CoT主要由两个阶段组成：
- **问题聚类**：将给定数据集的问题划分为几个簇；
- **演示采样**：从每个簇中选择一个代表性问题，并使用简单的启发式规则通过Zero-Shot-CoT生成其推理链。

### 问题聚类

由于基于多样性的聚类可能会减少由相似性导致的误导，我们针对给定的问题集合 $Q$ 执行聚类分析，。我们首先使用Sentence-BERT为 $Q$ 中的每个问题计算一个向量表示。随后，这些问题表示被 k-means 聚类算法处理，以产生 $k$ 个问题簇。对于簇 $i$ 中的每个问题，我们将它们按照到簇 $i$ 中心的距离升序排列成列表，$ q^{(i)} = [q_1^{(i)}, q_2^{(i)}, \ldots] $。

![](/img/Auto-CoT/cluester.png)

### 抽样并构造范例
在第二阶段，为那些抽样的问题生成推理链，然后选择符合我们选择标准的演示。具体来说，对于每个簇 $i$（$i = 1, \ldots, k$），我们构建一个演示 $d^{(i)}$（由一个问题、一个推理过程和一个答案组成的连接）。

对于簇 $i$，我们在排序后的列表$ q^{(i)} = [q_1^{(i)}, q_2^{(i)}, \ldots] $中迭代问题，直到满足我们的选择标准。换句话说，离簇 $i$ 中心更近的问题会被优先考虑。假设第 $j$ 近的问题 $q_j^{(i)}$ 正在被考虑，通过到零样本推理链，输入到大模型中，得到包含推理过程和提取的答案的推理链。然后，通过将问题、推理过程和答案连接起来，构建了第 $i$ 个簇的候选演示。

![](/img/Auto-CoT/construct.png)

### 增强后的上下文输入到大模型中
所构造的范例被用来增强测试问题的上下文学习。输入所有范例，然后送入LLM获得推理链，最后得到最终的答案。

## 论文实验

![](/img/Auto-CoT/result.png)
### 任务和数据集
论文在来自三个类别推理任务的十个基准数据集上进行了评估：

- 算术推理：此类别包括了多个数据集，如MultiArith、GSM8K、AddSub、AQUA-RAT、SingleEq 和 SVAMP。这些数据集旨在测试模型解决涉及基础数学运算问题的能力。
- 常识推理：包括CSQA 和 StrategyQA 数据集，它们用于评估模型理解并应用日常生活中非正式知识的能力。
- 符号推理：通过Last Letter Concatenation 和 Coin Flip 任务来检验模型处理抽象逻辑和模式识别的能力。
### 基线实验
- Zero-Shot 方法是指模型在没有见过任何特定任务示例的情况下直接尝试解决问题。
- Zero-Shot-CoT 是指在提问时给予模型一个“让我们一步一步思考”的提示，以激发其生成解题步骤。
- Few-Shot 方法则是提供少量相关任务的示例给模型学习，但不包含详细的解题过程。
- Manual-CoT 指的是人工构建的带有详细解题思路的示例，用以指导模型学习如何解决问题。
## 论文总结
这篇文章提出了自动思维链（Auto-CoT），先将问题聚类，每一类中抽出一个具有代表性的问题，利用零样本思维链为每个问题生成推理链。自动思维链（Auto-CoT）的优势在于，不需要手工分解解题步骤，不需要编写思维链提示词。这是一个很大的进步。但是，这种技术的也有一个明显的局限：需要对问题集划分成几个聚类，对每个聚类都要生成一组思维链提示词，并且这些提示词还要一并输入到大模型里。聚类太少，思维链的多样性不够；聚类太多，提示词太长，效率太低。